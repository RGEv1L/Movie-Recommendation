{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark Dynamodb fetch \"FindMyMovie\" personality records.\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    print(\"No Session\")\n",
    "#Starting PySpark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('data_processing')\\\n",
    "    .master(\"spark://192.168.1.100:7077\") \\\n",
    "    .config('spark.executor.memory','4600m') \\\n",
    "    .getOrCreate()   \n",
    "\n",
    "#Required imports other than SPARK main\n",
    "import csv\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StringType ,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull Date Dec 2 1030 AM\n",
    "\n",
    "#Dynamodb Handeler to connect to \"FindMyMovie\" table.\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "from botocore.exceptions import ClientError\n",
    "import botocore\n",
    "\n",
    "\n",
    "class   dynamodb_FindMyMovies():\n",
    "    def __init__(self):\n",
    "        # Get the service resource.\n",
    "        self.dynamodb = ''#boto3.resource('dynamodb',endpoint_url='http://localhost:8000')\n",
    "        self.table = ''#dynamodb.Table('FindMyMovies_Table')\n",
    "        \n",
    "\n",
    "    def check_state(self):\n",
    "        try:\n",
    "            self.dynamodb= boto3.resource('dynamodb',region_name='us-east-1',aws_access_key_id=\"Your Keys\",aws_secret_access_key=\"Your keys\")\n",
    "            self.table= self.dynamodb.Table('Your Table')\n",
    "            print(self.table.creation_date_time)\n",
    "            return 0\n",
    "        \n",
    "        except botocore.exceptions.ParamValidationError as e:\n",
    "            print(e)\n",
    "        except:\n",
    "            print(\"Couldn't connect with Database or Table\")\n",
    "            return 1\n",
    "        \n",
    "\n",
    "    def table_scan_register(self):\n",
    "        self.check_state()\n",
    "        response = self.table.scan(FilterExpression=Attr('register_date').gt('2019-1-1'))\n",
    "        return response\n",
    "    \n",
    "    def table_scan_combined_scores(self):\n",
    "        self.check_state()\n",
    "        response = self.table.scan(FilterExpression=Attr('r_type').eq('ipip120_response'))\n",
    "        return response\n",
    "                              \n",
    "    def table_scan_movies(self):\n",
    "        self.check_state()\n",
    "        response = self.table.scan(IndexName='titleID-LSI',FilterExpression=Attr('rating').gt(-1))\n",
    "        return response        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fetch New users with register date\n",
    "response_scan=dynamodb_FindMyMovies()\n",
    "e = response_scan.table_scan_register()\n",
    "\n",
    "dictionary = e['Items']\n",
    "\n",
    "#popping r_type , i-e metadata.\n",
    "for item in dictionary:\n",
    "    item.pop('r_type', None)\n",
    "    \n",
    "#saving metadata to csv.\n",
    "csv_columns = ['userID','full_name','email','country','gender','register_date','birthdate','age','ip_addr']\n",
    "dict_data = dictionary\n",
    "csv_file = \"/root/datasets/FYP/metadata.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read metadata csv and get userID and email column to get reference csv\n",
    "df=spark.read.csv('/root/datasets/FYP/metadata.csv',inferSchema=True,header=True)\n",
    "\n",
    "#Keeping only required columns to reference email and userID\n",
    "columns_to_drop = ['full_name', 'country','gender','register_date','birthdate','age','ip_addr']\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "df.columns\n",
    "df.head(2)\n",
    "\n",
    "#saving reference file\n",
    "#df.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('file:///home/tangr/output.csv')\n",
    "df.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/email_userID_ref.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 15:02:35.566000-04:00\n"
     ]
    }
   ],
   "source": [
    "# Fetching and converting email and combined scores to csv\n",
    "\n",
    "try:\n",
    "    del e\n",
    "except:\n",
    "    pass\n",
    "\n",
    "response_scan=dynamodb_FindMyMovies()\n",
    "e = response_scan.table_scan_combined_scores()\n",
    "e = e['Items']\n",
    "\n",
    "\n",
    "li = []\n",
    "new={}\n",
    "for x in e:\n",
    "    if x['email']:\n",
    "        if x['r_type'] == 'ipip120_response': \n",
    "            #Appending entries\n",
    "            new['email'] = x['email']\n",
    "            new['EXT'] = int(x['EXT'])\n",
    "            new['AGG'] = int(x['AGG'])\n",
    "            new['CON'] = int(x['CON'])\n",
    "            new['NEU'] = int(x['NEU'])\n",
    "            new['OPN'] = int(x['OPN'])\n",
    "            li.append(new.copy())\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "csv_columns = ['email','EXT','AGG','CON','NEU','OPN']\n",
    "dict_data = li\n",
    "csv_file = \"/root/datasets/FYP/combined_scores.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 15:02:35.566000-04:00\n"
     ]
    }
   ],
   "source": [
    "#Scaning LSI to get movies and ratings\n",
    "response_scan=dynamodb_FindMyMovies()\n",
    "e = response_scan.table_scan_movies()\n",
    "e = e['Items']\n",
    "\n",
    "\n",
    "csv_columns = ['email','titleID','rating','r_type']\n",
    "dict_data = e\n",
    "csv_file = \"/root/datasets/FYP/ratings.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleansing all files to replace emails with userIDs for privacy\n",
    "#Read metadata csv and get userID and email column to get reference csv\n",
    "\n",
    "#Cleaning metadata file\n",
    "df_meta=spark.read.csv('/root/datasets/FYP/metadata.csv',inferSchema=True,header=True)\n",
    "\n",
    "#Keeping only required columns to reference email and userID\n",
    "columns_to_drop = ['full_name','birthdate','age','ip_addr','email']\n",
    "df_meta = df_meta.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "df_meta.columns\n",
    "df_meta.head(2)\n",
    "\n",
    "#saving reference file\n",
    "#df.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('file:///home/tangr/output.csv')\n",
    "df_meta.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/meta.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading reference file and storing it as a python dict obj\n",
    "with open('/root/datasets/FYP/email_userID_ref.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        mydict = {rows[1]:rows[0] for rows in reader}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StringType ,IntegerType\n",
    "\n",
    "#Generic lookup function and register as udf\n",
    "def fetch_userID(email):\n",
    "        #a =df_ref.filter(df_ref['email']==email).collect()[0]\n",
    "        try:\n",
    "            return mydict[email]\n",
    "        except:\n",
    "            return 'default'\n",
    "get_udf=udf(fetch_userID,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing chaning combined scores files with transformation\n",
    "#loading files to be transformed for privacy\n",
    "df_scores=spark.read.csv('/root/datasets/FYP/combined_scores.csv',inferSchema=True,header=True)\n",
    "\n",
    "new_df = df_scores.withColumn('email',get_udf(df_scores.email))\n",
    "new_df = new_df.withColumnRenamed(\"email\", \"userID\")\n",
    "new_df.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/O/ipip_scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+\n",
      "| userID|EXT|AGG|CON|NEU|OPN|\n",
      "+-------+---+---+---+---+---+\n",
      "|default| 63| 81| 53| 70| 68|\n",
      "|default| 35| 68| 60| 64| 70|\n",
      "|default| 41| 58| 44| 88| 75|\n",
      "|default| 51| 72| 73| 65| 74|\n",
      "|default| 66| 62| 67| 66| 80|\n",
      "+-------+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing chaning combined scores files with transformation\n",
    "#loading files to be transformed for privacy\n",
    "df_scores=spark.read.csv('/root/datasets/FYP/ratings.csv',inferSchema=True,header=True)\n",
    "\n",
    "new_df = df_scores.withColumn('email',get_udf(df_scores.email))\n",
    "new_df = new_df.withColumnRenamed(\"email\", \"userID\")\n",
    "new_df.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/O/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+\n",
      "|              userID|titleID|rating|              r_type|\n",
      "+--------------------+-------+------+--------------------+\n",
      "|69dedd10-2371-405...| 299534|     8|2020-11-22 20:43:...|\n",
      "|fb3cc999-2cd2-466...|   1271|     7|2020-11-26 11:53:...|\n",
      "|fb3cc999-2cd2-466...|   1372|     9|2020-11-26 11:51:...|\n",
      "|fb3cc999-2cd2-466...|   1422|     9|2020-11-26 11:51:...|\n",
      "|fb3cc999-2cd2-466...| 146216|     6|2020-11-26 11:52:...|\n",
      "+--------------------+-------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         avg(EXT)|\n",
      "+-----------------+\n",
      "|61.84782608695652|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(OPN)|\n",
      "+-----------------+\n",
      "|68.90217391304348|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(AGG)|\n",
      "+-----------------+\n",
      "|70.55434782608695|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(NEU)|\n",
      "+-----------------+\n",
      "|62.55434782608695|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(OPN)|\n",
      "+-----------------+\n",
      "|68.90217391304348|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    variance(EXT)|\n",
      "+-----------------+\n",
      "|84.28428093645482|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    variance(OPN)|\n",
      "+-----------------+\n",
      "|70.74856665074051|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    variance(AGG)|\n",
      "+-----------------+\n",
      "|96.05195891065455|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    variance(NEU)|\n",
      "+-----------------+\n",
      "|140.0959149546107|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    variance(OPN)|\n",
      "+-----------------+\n",
      "|70.74856665074051|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Statisical Analysis of plain scores\n",
    "df_scores=spark.read.csv('/root/datasets/FYP/combined_scores.csv',inferSchema=True,header=True)\n",
    "\n",
    "df_scores.agg({'EXT': 'mean'}).show()\n",
    "df_scores.agg({'OPN': 'mean'}).show()\n",
    "df_scores.agg({'AGG': 'mean'}).show()\n",
    "df_scores.agg({'NEU': 'mean'}).show()\n",
    "df_scores.agg({'OPN': 'mean'}).show()\n",
    "df_scores.agg({'EXT': 'variance'}).show()\n",
    "df_scores.agg({'OPN': 'variance'}).show()\n",
    "df_scores.agg({'AGG': 'variance'}).show()\n",
    "df_scores.agg({'NEU': 'variance'}).show()\n",
    "df_scores.agg({'OPN': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+---+---+---+\n",
      "|              userID|OPN|AGG|EMO|CON|EXT|\n",
      "+--------------------+---+---+---+---+---+\n",
      "|8e7cebf9a234c064b...|5.0|2.0|3.0|2.5|6.5|\n",
      "|77c7d756a093150d4...|7.0|4.0|6.0|5.5|4.0|\n",
      "|b7e8a92987a530cc3...|4.0|3.0|4.5|2.0|2.5|\n",
      "|92561f21446e017dd...|5.5|5.5|4.0|4.5|4.0|\n",
      "|030001ac2145a938b...|5.5|5.5|3.5|4.5|2.5|\n",
      "+--------------------+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing impurity dataset and transforming it\n",
    "df_merge=spark.read.csv('/root/datasets/personality-isf2018/personality-data.csv',inferSchema=True,header=True)\n",
    "columns_to_drop = [' assigned metric',' assigned condition',' movie_1',\n",
    " ' predicted_rating_1',\n",
    " ' movie_2',\n",
    " ' predicted_rating_2',\n",
    " ' movie_3',\n",
    " ' predicted_rating_3',\n",
    " ' movie_4',\n",
    " ' predicted_rating_4',\n",
    " ' movie_5',\n",
    " ' predicted_rating_5',\n",
    " ' movie_6',\n",
    " ' predicted_rating_6',\n",
    " ' movie_7',\n",
    " ' predicted_rating_7',\n",
    " ' movie_8',\n",
    " ' predicted_rating_8',\n",
    " ' movie_9',\n",
    " ' predicted_rating_9',\n",
    " ' movie_10',\n",
    " ' predicted_rating_10',\n",
    " ' movie_11',\n",
    " ' predicted_rating_11',\n",
    " ' movie_12',\n",
    " ' predicted_rating_12',\n",
    " ' is_personalized',\n",
    " ' enjoy_watching ']\n",
    "df_merge = df_merge.drop(*columns_to_drop)\n",
    "df_merge = df_merge.withColumnRenamed(\"userid\", \"userID\")\n",
    "df_merge = df_merge.withColumnRenamed(\" openness\", \"OPN\")\n",
    "df_merge = df_merge.withColumnRenamed(\" agreeableness\", \"AGG\")\n",
    "df_merge = df_merge.withColumnRenamed(\" emotional_stability\", \"EMO\")\n",
    "df_merge = df_merge.withColumnRenamed(\" conscientiousness\", \"CON\")\n",
    "df_merge = df_merge.withColumnRenamed(\" extraversion\", \"EXT\")\n",
    "df_merge.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+---+---+---+\n",
      "|              userID|OPN|AGG|NEU|CON|EXT|\n",
      "+--------------------+---+---+---+---+---+\n",
      "|8e7cebf9a234c064b...|5.0|2.0|  5|2.5|6.5|\n",
      "|77c7d756a093150d4...|7.0|4.0|  2|5.5|4.0|\n",
      "|b7e8a92987a530cc3...|4.0|3.0|  3|2.0|2.5|\n",
      "|92561f21446e017dd...|5.5|5.5|  4|4.5|4.0|\n",
      "|030001ac2145a938b...|5.5|5.5|  4|4.5|2.5|\n",
      "+--------------------+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reversing EMOTIONAL STABILY to standard NEUROTICISM model schema\n",
    "def reverse(x):\n",
    "        return int(8-x) \n",
    "rev=udf(reverse,StringType())\n",
    "new_df = df_merge.withColumn('EMO',rev(df_merge.EMO))\n",
    "df_merge = new_df.withColumnRenamed(\"EMO\", \"NEU\")\n",
    "df_merge.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scailing all values from scale of 7 to scale of 100\n",
    "def scale(x):\n",
    "        return (x/7)*100\n",
    "scale_100=udf(scale,IntegerType())\n",
    "df_merge = df_merge.withColumn('OPN',scale(df_merge.OPN))\n",
    "df_merge = df_merge.withColumn('EXT',scale(df_merge.EXT))\n",
    "df_merge = df_merge.withColumn('AGG',scale(df_merge.AGG))\n",
    "df_merge = df_merge.withColumn('NEU',scale(df_merge.NEU))\n",
    "df_merge = df_merge.withColumn('CON',scale(df_merge.CON))\n",
    "\n",
    "df_merge = df_merge.withColumn(\"OPN\", df_merge.OPN.cast(IntegerType()))\n",
    "df_merge = df_merge.withColumn(\"AGG\", df_merge.AGG.cast(IntegerType()))\n",
    "df_merge = df_merge.withColumn(\"NEU\", df_merge.NEU.cast(IntegerType()))\n",
    "df_merge = df_merge.withColumn(\"EXT\", df_merge.EXT.cast(IntegerType()))\n",
    "df_merge = df_merge.withColumn(\"CON\", df_merge.CON.cast(IntegerType()))\n",
    "df_merge.show(2)\n",
    "df_merge.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/O/imp_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging original with impurity dataset\n",
    "df_1=spark.read.csv('/root/datasets/FYP/O/ipip_120_scores.csv',inferSchema=True,header=True)\n",
    "mega_df = df_1.union(df_merge)\n",
    "mega_df.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/O/agg/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=spark.read.csv('/root/datasets/FYP/O/agg/all_scores.csv',inferSchema=True,header=True)\n",
    "#Now adjust imported ratings scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a lookup for movieID to TMDB movie ID.\n",
    "#Loading reference file and storing it as a python dict obj\n",
    "mydict={}\n",
    "mydict2={}\n",
    "\n",
    "with open('/root/datasets/ml-20m/links.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        mydict = {rows[0]:rows[2] for rows in reader}\n",
    "\n",
    "with open('/root/datasets/ml-latest/links.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        mydict2 = {rows[0]:rows[2] for rows in reader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026765"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+\n",
      "|              userID|titleID|rating|              r_type|\n",
      "+--------------------+-------+------+--------------------+\n",
      "|8e7cebf9a234c064b...|    1.0|   5.0| 2001-09-10 17:19...|\n",
      "|8e7cebf9a234c064b...|    2.0|   4.0| 2001-09-28 11:34...|\n",
      "|8e7cebf9a234c064b...|    3.0|   4.0| 2001-09-28 11:42...|\n",
      "|8e7cebf9a234c064b...|    5.0|   5.0| 2001-09-28 11:27...|\n",
      "|8e7cebf9a234c064b...|    6.0|   4.0| 2002-01-07 18:12...|\n",
      "+--------------------+-------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mov=spark.read.csv('/root/datasets/personality-isf2018/ratings.csv',inferSchema=True,header=True)\n",
    "df_mov = df_mov.withColumnRenamed(\"useri\", \"userID\")\n",
    "df_mov = df_mov.withColumnRenamed(\" tstamp \", \"r_type\")\n",
    "df_mov = df_mov.withColumnRenamed(\" movie_id\", \"titleID\")\n",
    "df_mov = df_mov.withColumnRenamed(\" rating\", \"rating\")\n",
    "\n",
    "\n",
    "\n",
    "df_mov.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mov = df_mov.withColumn(\"rating\", df_mov[\"rating\"].cast(\"integer\"))\n",
    "df_mov = df_mov.withColumn(\"titleID\", df_mov[\"titleID\"].cast(\"integer\"))\n",
    "df_mov = df_mov.withColumn(\"titleID\", df_mov[\"titleID\"].cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"titleID is NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting titleIDs to TMDB Ids by writing UDF and searching in lookup dict\n",
    "\n",
    "def tmdb_conv(x):\n",
    "    a = mydict.get(x,None)\n",
    "    if a is None:\n",
    "        b = mydict2.get(x,None)\n",
    "        return b\n",
    "    else:\n",
    "        return a \n",
    "tmdb_udf=udf(tmdb_conv,StringType())\n",
    "\n",
    "df_new = df_mov.withColumn('titleID',tmdb_udf(df_mov.titleID))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026765"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_new.filter(df_new.titleID. isNotNull())\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025835"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df.withColumn(\"titleID\", df[\"titleID\"].cast(\"integer\"))\n",
    "df = df.filter(df.titleID. isNotNull())\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+\n",
      "|userID|titleID|rating|r_type|\n",
      "+------+-------+------+------+\n",
      "+------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_DF.filter(\"titleID is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rating_conv(x):\n",
    "    x = x / 5\n",
    "    x = x* 10\n",
    "    x = int(x)\n",
    "    return x\n",
    "\n",
    "rat_udf=udf(rating_conv,IntegerType())\n",
    "\n",
    "new_DF = df.withColumn('rating',rat_udf(df.rating))\n",
    "new_DF.filter(\"titleID is NULL\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.summary of DataFrame[userID: string, titleID: int, rating: int, r_type: string]>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_DF.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_DF \\\n",
    "   .repartition(1) \\\n",
    "   .write.format(\"com.databricks.spark.csv\") \\\n",
    "   .option(\"header\", \"true\")  \\\n",
    "   .save(\"/root/datasets/FYP/O/agg/ratings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a movie reference file\n",
    "\n",
    "#Loading reference file and storing it as a python dict obj\n",
    "with open('/root/datasets/ml-20m/links.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        mydict = {rows[0]:rows[2] for rows in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies=spark.read.csv('/root/datasets/ml-20m/movies.csv',inferSchema=True,header=True)\n",
    "#Converting titleIDs to TMDB Ids by writing UDF and searching in lookup dict\n",
    "def get_mov(x):\n",
    "    try:\n",
    "        return mydict[x]\n",
    "    except:\n",
    "        return '0'\n",
    "\n",
    "get=udf(get_mov,StringType())\n",
    "df_movies = df_movies.withColumn(\"movieId\", df_movies[\"movieId\"].cast(\"string\"))\n",
    "df_movies = df_movies.withColumn('movieId',get(df_movies.movieId))\n",
    "df_movies = df_movies.withColumn(\"movieId\", df_movies[\"movieId\"].cast(\"integer\"))\n",
    "df_movies = df_movies.withColumnRenamed(\"movieId\", \"titleID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_movies.write.format('csv').option('header',True).option('sep',',').save('/root/datasets/FYP/O/agg/movies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|titleID|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|    862|    Toy Story (1995)|Adventure|Animati...|\n",
      "|   8844|      Jumanji (1995)|Adventure|Childre...|\n",
      "|  15602|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|  31357|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|  11862|Father of the Bri...|              Comedy|\n",
      "|    949|         Heat (1995)|Action|Crime|Thri...|\n",
      "|  11860|      Sabrina (1995)|      Comedy|Romance|\n",
      "|  45325| Tom and Huck (1995)|  Adventure|Children|\n",
      "|   9091| Sudden Death (1995)|              Action|\n",
      "|    710|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|   9087|American Presiden...|Comedy|Drama|Romance|\n",
      "|  12110|Dracula: Dead and...|       Comedy|Horror|\n",
      "|  21032|        Balto (1995)|Adventure|Animati...|\n",
      "|  10858|        Nixon (1995)|               Drama|\n",
      "|   1408|Cutthroat Island ...|Action|Adventure|...|\n",
      "|    524|       Casino (1995)|         Crime|Drama|\n",
      "|   4584|Sense and Sensibi...|       Drama|Romance|\n",
      "|      5|   Four Rooms (1995)|              Comedy|\n",
      "|   9273|Ace Ventura: When...|              Comedy|\n",
      "|  11517|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_movies=spark.read.csv('/root/datasets/FYP/O/agg/ratings/orgrating.csv',inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.summary of DataFrame[userID: string, titleID: int, rating: int, r_type: string]>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
